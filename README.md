# End-to-end LLM implementation

Followed GPT's baseline architecture. The system is divided into 3 parts-
1. Preprocessing
2. Pretraining
3. Finetuning

### Preprocessing
1. Text pre-processing: Cleaning the text with regex
2. Tokenization: Implemented word tokenization, character tokenization and Byte-pair encoding (Used in GPT). BPE is implemented using tikoken
3. Vector Embedding:

### Pretraining
1. Simplified Attention
2. Self Attention
3. Cross Attention
4. Multihead attention
