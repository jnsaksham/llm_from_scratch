# End-to-end LLM implementation

Followed GPT's baseline architecture. The system is divided into 3 parts-
1. Preprocessing
2. Pretraining
3. Finetuning

### Preprocessing
Text pre-processing: Cleaning the text with regex
Tokenization: Implemented word tokenization, character tokenization and Byte-pair encoding (Used in GPT). BPE is implemented using tikoken.
Vector Embedding: 

### Pretraining
Simplified Attention
Self Attention
Cross Attention
Multihead attention
