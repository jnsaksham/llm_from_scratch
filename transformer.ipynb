{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dostoyevsky Transformer\n",
    "\n",
    "Notebook for training a transformer model on Dostoyevsky's books\\\n",
    "\\\n",
    "Source for books: https://www.gutenberg.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from services.dataloader import load_txt\n",
    "from services.tokenizer import VocabBuilder, WordTokenizer, CharacterTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenization is the process to convert text into tokens. 3 types of tokenizers have been added- \\\n",
    "1. Word tokenizer - each word and each character is considered as a separate token\n",
    "2. Char tokenizer - each char is considered as a separate token\n",
    "3. TODO: BPE tokenizer - Byte-pair encoding. Used by ChatGPT (token size ~ 50K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data corpus directory\n",
    "books_dir = 'books'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single book tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the first book\n",
    "book_name = os.listdir(books_dir)[0]\n",
    "\n",
    "txt = load_book(os.path.join(books_dir, book_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1947970"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 16327\n"
     ]
    }
   ],
   "source": [
    "vocab_builder = VocabBuilder()\n",
    "vocab = vocab_builder.create_word_vocab(txt)\n",
    "print(f'vocab size: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "word_tokenizer = WordTokenizer(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[493, 6622, 1375, 674, 8206, 6945, 16325, 2370, 4054, 14837, 4356, 13715, 11123]\n",
      "Everything from Project Gutenberg is gratis <|unk|> and completely without cost to readers\n"
     ]
    }
   ],
   "source": [
    "test_string = \" Everything from Project Gutenberg is gratis, libre, and completely without cost to readers.\"\n",
    "encoded_string = word_tokenizer.encode(test_string)\n",
    "print(encoded_string)\n",
    "\n",
    "decoded_string = word_tokenizer.decode(encoded_string)\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Character Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character vocab size: 103\n"
     ]
    }
   ],
   "source": [
    "char_vocab = vocab_builder.create_character_vocab(txt)\n",
    "print(f'character vocab size: {len(char_vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_tokenizer = CharacterTokenizer(char_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 28, 74, 57, 70, 77, 72, 60, 61, 66, 59, 1, 58, 70, 67, 65, 1, 39, 70, 67, 62, 57, 55, 72, 1, 30, 73, 72, 57, 66, 54, 57, 70, 59, 1, 61, 71, 1, 59, 70, 53, 72, 61, 71, 7, 1, 64, 61, 54, 70, 57, 7, 1, 53, 66, 56, 1, 55, 67, 65, 68, 64, 57, 72, 57, 64, 77, 1, 75, 61, 72, 60, 67, 73, 72, 1, 55, 67, 71, 72, 1, 72, 67, 1, 70, 57, 53, 56, 57, 70, 71, 9]\n",
      " Everything from Project Gutenberg is gratis, libre, and completely without cost to readers.\n"
     ]
    }
   ],
   "source": [
    "test_string = \" Everything from Project Gutenberg is gratis, libre, and completely without cost to readers.\"\n",
    "encoded_string = char_tokenizer.encode(test_string)\n",
    "print(encoded_string)\n",
    "\n",
    "decoded_string = char_tokenizer.decode(encoded_string)\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11391, 422, 4935, 20336, 318, 14586, 271, 11, 9195, 260, 11, 290, 3190, 1231, 1575, 284, 7183, 13, 220, 50256, 1374, 389, 345, 30]\n",
      " Everything from Project Gutenberg is gratis, libre, and completely without cost to readers. <|endoftext|> How are you?\n"
     ]
    }
   ],
   "source": [
    "# Testing the tokenizer\n",
    "test_string = \" Everything from Project Gutenberg is gratis, libre, and completely without cost to readers. <|endoftext|> How are you?\"\n",
    "encoded_string = bpe_tokenizer.encode(test_string, allowed_special={\"<|endoftext|>\"})\n",
    "print(encoded_string)\n",
    "\n",
    "decoded_string = bpe_tokenizer.decode(encoded_string)\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the books corpus and tokenize using BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\n",
    "for book_name in os.listdir(books_dir):\n",
    "    corpus = corpus + load_book(os.path.join(books_dir, book_name)) + \"<|endoftext|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = bpe_tokenizer.encode(corpus, allowed_special={\"<|endoftext|>\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input-output pairs\n",
    "Concatenated multiple books by Fyodor \\\n",
    "Using BPE Encoding from now on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
