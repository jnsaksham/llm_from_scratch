{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dostoyevsky Transformer\n",
    "\n",
    "Notebook for training a transformer model on Dostoyevsky's books\\\n",
    "\\\n",
    "Source for books: https://www.gutenberg.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.data.dataloader import load_txt\n",
    "from src.preprocess.tokenizer import VocabBuilder, WordTokenizer, CharacterTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenization is the process to convert text into tokens. 3 types of tokenizers have been added- \\\n",
    "1. Word tokenizer - each word and each character is considered as a separate token\n",
    "2. Char tokenizer - each char is considered as a separate token\n",
    "3. TODO: BPE tokenizer - Byte-pair encoding. Used by ChatGPT (token size ~ 50K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data corpus directory\n",
    "books_dir = os.path.join('..', 'books')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single book tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num chars: 1947970\n"
     ]
    }
   ],
   "source": [
    "# select the first book\n",
    "book_name = os.listdir(books_dir)[0]\n",
    "\n",
    "txt = load_txt(os.path.join(books_dir, book_name))\n",
    "print(f'num chars: {len(txt)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 16327\n"
     ]
    }
   ],
   "source": [
    "vocab_builder = VocabBuilder()\n",
    "vocab = vocab_builder.create_word_vocab(txt)\n",
    "print(f'vocab size: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "word_tokenizer = WordTokenizer(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[493, 6622, 1375, 674, 8206, 6945, 16325, 2370, 4054, 14837, 4356, 13715, 11123]\n",
      "Everything from Project Gutenberg is gratis <|unk|> and completely without cost to readers\n"
     ]
    }
   ],
   "source": [
    "test_string = \" Everything from Project Gutenberg is gratis, libre, and completely without cost to readers.\"\n",
    "encoded_string = word_tokenizer.encode(test_string)\n",
    "print(encoded_string)\n",
    "\n",
    "decoded_string = word_tokenizer.decode(encoded_string)\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Character Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character vocab size: 103\n"
     ]
    }
   ],
   "source": [
    "char_vocab = vocab_builder.create_character_vocab(txt)\n",
    "print(f'character vocab size: {len(char_vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_tokenizer = CharacterTokenizer(char_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 28, 74, 57, 70, 77, 72, 60, 61, 66, 59, 1, 58, 70, 67, 65, 1, 39, 70, 67, 62, 57, 55, 72, 1, 30, 73, 72, 57, 66, 54, 57, 70, 59, 1, 61, 71, 1, 59, 70, 53, 72, 61, 71, 7, 1, 64, 61, 54, 70, 57, 7, 1, 53, 66, 56, 1, 55, 67, 65, 68, 64, 57, 72, 57, 64, 77, 1, 75, 61, 72, 60, 67, 73, 72, 1, 55, 67, 71, 72, 1, 72, 67, 1, 70, 57, 53, 56, 57, 70, 71, 9]\n",
      " Everything from Project Gutenberg is gratis, libre, and completely without cost to readers.\n"
     ]
    }
   ],
   "source": [
    "test_string = \" Everything from Project Gutenberg is gratis, libre, and completely without cost to readers.\"\n",
    "encoded_string = char_tokenizer.encode(test_string)\n",
    "print(encoded_string)\n",
    "\n",
    "decoded_string = char_tokenizer.decode(encoded_string)\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11391, 422, 4935, 20336, 318, 14586, 271, 11, 9195, 260, 11, 290, 3190, 1231, 1575, 284, 7183, 13, 220, 50256, 1374, 389, 345, 30]\n",
      " Everything from Project Gutenberg is gratis, libre, and completely without cost to readers. <|endoftext|> How are you?\n"
     ]
    }
   ],
   "source": [
    "# Testing the tokenizer\n",
    "test_string = \" Everything from Project Gutenberg is gratis, libre, and completely without cost to readers. <|endoftext|> How are you?\"\n",
    "encoded_string = bpe_tokenizer.encode(test_string, allowed_special={\"<|endoftext|>\"})\n",
    "print(encoded_string)\n",
    "\n",
    "decoded_string = bpe_tokenizer.decode(encoded_string)\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the books corpus and tokenize using BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\n",
    "for book_name in os.listdir(books_dir):\n",
    "    corpus = corpus + load_txt(os.path.join(books_dir, book_name)) + \"<|endoftext|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = bpe_tokenizer.encode(corpus, allowed_special={\"<|endoftext|>\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input-output pairs\n",
    "Concatenated multiple books by Fyodor \\\n",
    "Using BPE Encoding from here on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.dataloader import create_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader(txt, max_length=4, stride=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Embeddings\n",
    "Using word2vec implemented in the gensim library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model\n",
    "import gensim.downloader as api\n",
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding dim:  300\n"
     ]
    }
   ],
   "source": [
    "word_vectors = model\n",
    "\n",
    "print('embedding dim: ', len(word_vectors['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(word_vectors.most_similar(positive=['king', 'woman'], negative=['man'], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76640123\n",
      "0.6510956\n",
      "0.7901483\n",
      "0.080210164\n",
      "-0.0060949083\n"
     ]
    }
   ],
   "source": [
    "print(word_vectors.similarity('woman', 'man'))\n",
    "print(word_vectors.similarity('king', 'queen'))\n",
    "print(word_vectors.similarity('mother', 'father'))\n",
    "print(word_vectors.similarity('rock', 'paper'))\n",
    "print(word_vectors.similarity('rock', 'mitochondria'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
